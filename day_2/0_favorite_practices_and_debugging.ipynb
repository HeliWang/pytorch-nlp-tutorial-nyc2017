{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One way to software engineer the problem\n",
    "\n",
    "There are a few different ways of breaking up the processing pipeline for handling language data. In this notebook, we will be operating under the following division of labor:\n",
    "\n",
    "- Vocabulary\n",
    "    + A vocabulary class is responsible managing the bijections between string tokens and integers.\n",
    "    + It offers the follow API:\n",
    "        + `add`, indexing with `[]`, `add_many` will take as input a string and return an integer (or a sequence of strings and a sequence of integers in the case of `add_many`\n",
    "        + `lookup` takes as input an integer and returns the corresponding string toekn\n",
    "    + It also handles the restriction of observed tokens that did not happen frequently enough. We will be restricting on the frequency of tokens rather than taking the N most frequent. This is because our primary concern is with statistical signal, not raw processing complexity.\n",
    "- TextDataset\n",
    "    + TextDataset is responsible for loading the data from disk.  it should process the data to a standardized, intermediate form\n",
    "- Vectorizer\n",
    "    + The Vectorizer uses an existing TextDataset to instantiate one or more vocabularies (more than one is needed if dealing with different models or category labels).  \n",
    "    + Using the vocabs, it can convert any existing TextDataset to a VectorizedDataset, which interfaces with the learner\n",
    "    + It is important to note the following pipeline:\n",
    "        1. Load the Training TextDataset\n",
    "        2. Load the Testing (Or Dev/Eval) TextDataset\n",
    "        2. Instantiate the Vectorizer\n",
    "            + in this process, the vectorizer will be frozen, so it cannot grow its token-integer mapping\n",
    "            + this is to preserve realistic testing conditions\n",
    "        3. Use Vectorizer to create a VectorizedDataset from the Training TextDataset\n",
    "        4. Use Vectorizer to create another VectorizedDataset from the Testing (or Dev/Eval) TextDataset\n",
    "- VectorizedDataset\n",
    "    + A VectorizedDataset has the necessary structures required for learning\n",
    "    + A VectorizedDataset is best created all at once from the vectorizer\n",
    "    + It is also a computation that can be pre-cached for speed\n",
    "- DataLoader\n",
    "    + The DataLoader uses the VectorizedDataset to issue batches of data. \n",
    "    + The DataLoader should be issuing randomized batches so that the model doesn't get entrenched on a specific pattern of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    An implementation that manages the interface between a token dataset and the\n",
    "        machine learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_unks=False, unk_token=\"<UNK>\",\n",
    "                 use_mask=False, mask_token=\"<MASK>\", use_start_end=False,\n",
    "                 start_token=\"<START>\", end_token=\"<END>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_unks (bool): The vocabulary will output UNK tokens for out of\n",
    "                vocabulary items.\n",
    "                [default=False]\n",
    "            unk_token (str): The token used for unknown tokens.\n",
    "                If `use_unks` is True, this will be added to the vocabulary.\n",
    "                [default='<UNK>']\n",
    "            use_mask (bool): The vocabulary will reserve the 0th index for a mask token.\n",
    "                This is used to handle variable lengths in sequence models.\n",
    "                [default=False]\n",
    "            mask_token (str): The token used for the mask.\n",
    "                Note: mostly a placeholder; it's unlikely the token will be seen.\n",
    "                [default='<MASK>']\n",
    "            use_start_end (bool): The vocabulary will reserve indices for two tokens\n",
    "                that represent the start and end of a sequence.\n",
    "                [default=False]\n",
    "            start_token: The token used to indicate the start of a sequence.\n",
    "                If `use_start_end` is True, this will be added to the vocabulary.\n",
    "                [default='<START>']\n",
    "            end_token: The token used to indicate the end of a sequence\n",
    "                 If `use_start_end` is True, this will be added to the vocabulary.\n",
    "                 [default='<END>']\n",
    "        \"\"\"\n",
    "\n",
    "        self._mapping = {}  # str -> int\n",
    "        self._flip = {}  # int -> str;\n",
    "        self._counts = Counter()  # int -> int; count occurrences\n",
    "        self._forced_unks = set()  # force tokens to unk (e.g. if < 5 occurrences)\n",
    "        self._i = 0\n",
    "        self._frozen = False\n",
    "        self._frequency_threshold = -1\n",
    "\n",
    "        # mask token for use in masked recurrent networks\n",
    "        # usually need to be the 0th index\n",
    "        self.use_mask = use_mask\n",
    "        self.mask_token = mask_token\n",
    "        if self.use_mask:\n",
    "            self.add(self.mask_token)\n",
    "\n",
    "        # unk token for out of vocabulary tokens\n",
    "        self.use_unks = use_unks\n",
    "        self.unk_token = unk_token\n",
    "        if self.use_unks:\n",
    "            self.add(self.unk_token)\n",
    "\n",
    "        # start token for sequence models\n",
    "        self.use_start_end = use_start_end\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        if self.use_start_end:\n",
    "            self.add(self.start_token)\n",
    "            self.add(self.end_token)\n",
    "\n",
    "    def iterkeys(self):\n",
    "        for k in self._mapping.keys():\n",
    "            if k == self.unk_token or k == self.mask_token:\n",
    "                continue\n",
    "            else:\n",
    "                yield k\n",
    "\n",
    "    def keys(self):\n",
    "        return list(self.iterkeys())\n",
    "\n",
    "    def iteritems(self):\n",
    "        for key, value in self._mapping.items():\n",
    "            if key == self.unk_token or key == self.mask_token:\n",
    "                continue\n",
    "            yield key, value\n",
    "\n",
    "    def items(self):\n",
    "        return list(self.iteritems())\n",
    "\n",
    "    def values(self):\n",
    "        return [value for _, value in self.iteritems()]\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if self._frozen:\n",
    "            if k in self._mapping:\n",
    "                out_index = self._mapping[k]\n",
    "            elif self.use_unks:\n",
    "                out_index = self.unk_index\n",
    "            else:  # case: frozen, don't want unks, raise exception\n",
    "                raise VocabularyException(\"Vocabulary is frozen. \" +\n",
    "                                          \"Key '{}' not found.\".format(k))\n",
    "            if out_index in self._forced_unks:\n",
    "                out_index = self.unk_index\n",
    "        elif k in self._mapping:  # case: normal\n",
    "            out_index = self._mapping[k]\n",
    "            self._counts[out_index] += 1\n",
    "        else:\n",
    "            out_index = self._mapping[k] = self._i\n",
    "            self._i += 1\n",
    "            self._flip[out_index] = k\n",
    "            self._counts[out_index] = 1\n",
    "\n",
    "        return out_index\n",
    "\n",
    "    def add(self, k):\n",
    "        return self.__getitem__(k)\n",
    "\n",
    "    def add_many(self, x):\n",
    "        return [self.add(k) for k in x]\n",
    "\n",
    "    def lookup(self, i):\n",
    "        try:\n",
    "            return self._flip[i]\n",
    "        except KeyError:\n",
    "            raise VocabularyException(\"Key {} not in Vocabulary\".format(i))\n",
    "\n",
    "    def lookup_many(self, x):\n",
    "        for k in x:\n",
    "            yield self.lookup(k)\n",
    "\n",
    "    def map(self, sequence, include_start_end=False):\n",
    "        if include_start_end:\n",
    "            yield self.start_index\n",
    "\n",
    "        for item in sequence:\n",
    "            yield self[item]\n",
    "\n",
    "        if include_start_end:\n",
    "            yield self.end_index\n",
    "\n",
    "    def freeze(self, use_unks=False, frequency_cutoff=-1):\n",
    "        self.use_unks = use_unks\n",
    "        self._frequency_cutoff = frequency_cutoff\n",
    "\n",
    "        if use_unks and self.unk_token not in self:\n",
    "            self.add(self.unk_token)\n",
    "\n",
    "        if self._frequency_cutoff > 0:\n",
    "            for token, count in self._counts.items():\n",
    "                if count < self._frequency_cutoff:\n",
    "                    self._forced_unks.add(token)\n",
    "\n",
    "        self._frozen = True\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self._frozen = False\n",
    "\n",
    "    def get_counts(self):\n",
    "        return {self._flip[i]: count for i, count in self._counts.items()}\n",
    "\n",
    "    def get_count(self, token=None, index=None):\n",
    "        if token is None and index is None:\n",
    "            return None\n",
    "        elif token is not None and index is not None:\n",
    "            print(\"Cannot do two things at once; choose one\")\n",
    "        elif token is not None:\n",
    "            return self._counts[self[token]]\n",
    "        elif index is not None:\n",
    "            return self._counts[index]\n",
    "        else:\n",
    "            raise Exception(\"impossible condition\")\n",
    "\n",
    "    @property\n",
    "    def unk_index(self):\n",
    "        if self.unk_token not in self:\n",
    "            return None\n",
    "        return self._mapping[self.unk_token]\n",
    "\n",
    "    @property\n",
    "    def mask_index(self):\n",
    "        if self.mask_token not in self:\n",
    "            return None\n",
    "        return self._mapping[self.mask_token]\n",
    "\n",
    "    @property\n",
    "    def start_index(self):\n",
    "        if self.start_token not in self:\n",
    "            return None\n",
    "        return self._mapping[self.start_token]\n",
    "\n",
    "    @property\n",
    "    def end_index(self):\n",
    "        if self.end_token not in self:\n",
    "            return None\n",
    "        return self._mapping[self.end_token]\n",
    "\n",
    "    def __contains__(self, k):\n",
    "        return k in self._mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._mapping)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Vocabulary(size={},frozen={})>\".format(len(self), self._frozen)\n",
    "\n",
    "\n",
    "    def get_serializable_contents(self):\n",
    "        \"\"\"\n",
    "        Creats a dict containing the necessary information to recreate this instance\n",
    "        \"\"\"\n",
    "        config = {\"_mapping\": self._mapping,\n",
    "                  \"_flip\": self._flip,\n",
    "                  \"_frozen\": self._frozen,\n",
    "                  \"_i\": self._i,\n",
    "                  \"_counts\": list(self._counts.items()),\n",
    "                  \"_frequency_threshold\": self._frequency_threshold,\n",
    "                  \"use_unks\": self.use_unks,\n",
    "                  \"unk_token\": self.unk_token,\n",
    "                  \"use_mask\": self.use_mask,\n",
    "                  \"mask_token\": self.mask_token,\n",
    "                  \"use_start_end\": self.use_start_end,\n",
    "                  \"start_token\": self.start_token,\n",
    "                  \"end_token\": self.end_token}\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def deserialize_from_contents(cls, content):\n",
    "        \"\"\"\n",
    "        Recreate a Vocabulary instance; expect same dict as output in `serialize`\n",
    "        \"\"\"\n",
    "        try:\n",
    "            _mapping = content.pop(\"_mapping\")\n",
    "            _flip = content.pop(\"_flip\")\n",
    "            _i = content.pop(\"_i\")\n",
    "            _frozen = content.pop(\"_frozen\")\n",
    "            _counts = content.pop(\"_counts\")\n",
    "            _frequency_threshold = content.pop(\"_frequency_threshold\")\n",
    "        except KeyError:\n",
    "            raise Exception(\"unable to deserialize vocabulary\")\n",
    "        if isinstance(list(_flip.keys())[0], six.string_types):\n",
    "            _flip = {int(k): v for k, v in _flip.items()}\n",
    "        out = cls(**content)\n",
    "        out._mapping = _mapping\n",
    "        out._flip = _flip\n",
    "        out._i = _i\n",
    "        out._counts = Counter(dict(_counts))\n",
    "        out._frequency_threshold = _frequency_threshold\n",
    "\n",
    "        if _frozen:\n",
    "            out.freeze(out.use_unks)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the names dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class constants:\n",
    "    # usually this would be some sort of constants.py or something similar\n",
    "    NAMES_TRAIN = '/research/data/names/names_train.csv'\n",
    "    NAMES_TEST = '/research/data/names/names_test.csv'\n",
    "    CHAR_START_TOKEN = \"^\"\n",
    "    CHAR_END_TOKEN= \"_\"\n",
    "\n",
    "class NamesDataset(object):\n",
    "    def __init__(self, data_path, delimiter=\"\\t\"):\n",
    "        self.data = pd.read_csv(data_path, delimiter=delimiter)\n",
    "        \n",
    "    def get_data(self, nationality=None):\n",
    "        data = self.data\n",
    "        if nationality:\n",
    "            data = self.data[self.data['label']==nationality]\n",
    "        return data['name'].values, data['label'].values\n",
    "    \n",
    "\n",
    "class NamesVectorizer(object):\n",
    "    def __init__(self, chars_vocab, targets_vocab, max_sequence_length):\n",
    "        self.chars_vocab = chars_vocab\n",
    "        self.targets_vocab = targets_vocab\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    @classmethod\n",
    "    def induce_from_text(cls, text, targets):\n",
    "        chars_vocab = Vocabulary(use_unks=True,\n",
    "                                 use_start_end=True,\n",
    "                                 start_token=constants.CHAR_START_TOKEN,\n",
    "                                 end_token=constants.CHAR_END_TOKEN)\n",
    "        \n",
    "        targets_vocab = Vocabulary(use_unks=False,\n",
    "                                   use_start_end=False)\n",
    "        \n",
    "        for character_sequence in text:\n",
    "            chars_vocab.add_many(set(character_sequence))\n",
    "\n",
    "        targets_vocab.add_many(targets)\n",
    "\n",
    "        # add two for the start, end tokens\n",
    "        max_sequence_length = 2 + max(len(character_sequence) for character_sequence in text)\n",
    "\n",
    "        return cls(chars_vocab, targets_vocab, max_sequence_length)\n",
    "\n",
    "    def convert_dataset(self, char_sequences, targets):\n",
    "        num_data = len(char_sequences)\n",
    "        \n",
    "        # create the intended output structures\n",
    "        x_seq = np.zeros((num_data, self.max_sequence_length), dtype=np.int64)\n",
    "        y_target = np.zeros((num_data), dtype=np.int64)\n",
    "\n",
    "        # iterate our targets and sequences until they are populated \n",
    "        \n",
    "        for seq_i, target in enumerate(targets):\n",
    "            y_target[seq_i] = self.targets_vocab[target]\n",
    "\n",
    "        for seq_i, char_seq in enumerate(char_sequences):\n",
    "            converted_seq = list(self.chars_vocab.map(char_seq, include_start_end=True))\n",
    "            x_seq[seq_i, :len(converted_seq)] = converted_seq\n",
    "\n",
    "        return VectorizedSingleIODataset(x_seq, y_target)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"chars_vocab\": vectorizer.chars_vocab.get_serializable_contents(),\n",
    "                    \"targets_vocab\": vectorizer.targets_vocab.get_serializable_contents(),\n",
    "                    \"max_sequence_length\": vectorizer.max_sequence_length()}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"chars_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"chars_vocab\"])\n",
    "        vec_dict[\"targets_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"targets_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "        \n",
    "class VectorizedSingleIODataset(Dataset):\n",
    "    def __init__(self, x_input, y_target):\n",
    "        self.x_input = x_input\n",
    "        self.y_target = y_target\n",
    "        self.class_weights = 1 / np.bincount(self.y_target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_input': self.x_input[index],\n",
    "                'y_target': self.y_target[index], \n",
    "                'class_weights': self.class_weights,\n",
    "                'x_seq_lengths': len(self.x_input[index].nonzero()[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataServer(object):\n",
    "    def __init__(self, vectorized_data):\n",
    "        self.vectorized_data = vectorized_data\n",
    "        self.gpu_mode = False\n",
    "        self.volatile_mode = False\n",
    "\n",
    "    def serve_batches(self, batch_size, num_batches=-1, num_workers=0):\n",
    "        datagen = DataLoader(self.vectorized_data, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=num_workers)\n",
    "        for batch_index, batch in enumerate(datagen):\n",
    "            out = {}\n",
    "            for key, val in batch.items():\n",
    "                if not isinstance(val, Variable):\n",
    "                    val = Variable(val)\n",
    "                if self.gpu_mode:\n",
    "                    val = val.cuda()\n",
    "                if self.volatile_mode:\n",
    "                    val = val.volatile()\n",
    "                out[key] = val\n",
    "\n",
    "            yield out\n",
    "            if num_batches > 0 and batch_index > num_batches:\n",
    "                break\n",
    "\n",
    "    def enable_gpu_mode(self):\n",
    "        self.gpu_mode = True\n",
    "\n",
    "    def disable_gpu_mode(self):\n",
    "        self.gpu_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NamesDataset(constants.NAMES_TRAIN)\n",
    "train_names, train_targets = train_dataset.get_data()\n",
    "\n",
    "test_dataset = NamesDataset(constants.NAMES_TEST)\n",
    "test_names, test_targets = test_dataset.get_data()\n",
    "\n",
    "vectorizer = NamesVectorizer.induce_from_text(list(train_names) + list(test_names), \n",
    "                                              list(train_targets) + list(test_targets))\n",
    "\n",
    "vectorized_train = vectorizer.convert_dataset(train_names, train_targets)\n",
    "train_server = DataServer(vectorized_train)\n",
    "\n",
    "vectorized_test = vectorizer.convert_dataset(test_names, test_targets)\n",
    "test_server = DataServer(vectorized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weights': Variable containing:\n",
       " \n",
       " Columns 0 to 9 \n",
       " 1.00000e-02 *\n",
       "   0.0624  0.0132  0.2445  0.1224  0.0343  0.1767  1.5873  0.1779  0.6211  0.4525\n",
       "   0.0624  0.0132  0.2445  0.1224  0.0343  0.1767  1.5873  0.1779  0.6211  0.4525\n",
       "   0.0624  0.0132  0.2445  0.1224  0.0343  0.1767  1.5873  0.1779  0.6211  0.4525\n",
       "   0.0624  0.0132  0.2445  0.1224  0.0343  0.1767  1.5873  0.1779  0.6211  0.4525\n",
       "   0.0624  0.0132  0.2445  0.1224  0.0343  0.1767  1.5873  0.1779  0.6211  0.4525\n",
       " \n",
       " Columns 10 to 17 \n",
       " 1.00000e-02 *\n",
       "   0.4255  0.5319  0.4785  0.4292  1.4286  1.2346  1.6949  0.9259\n",
       "   0.4255  0.5319  0.4785  0.4292  1.4286  1.2346  1.6949  0.9259\n",
       "   0.4255  0.5319  0.4785  0.4292  1.4286  1.2346  1.6949  0.9259\n",
       "   0.4255  0.5319  0.4785  0.4292  1.4286  1.2346  1.6949  0.9259\n",
       "   0.4255  0.5319  0.4785  0.4292  1.4286  1.2346  1.6949  0.9259\n",
       " [torch.DoubleTensor of size 5x18], 'x_input': Variable containing:\n",
       " \n",
       " Columns 0 to 12 \n",
       "     1    46    10    13    26     2     0     0     0     0     0     0     0\n",
       "     1    35    13    10     8    24    14     5    11     8     7     2     0\n",
       "     1     6    18    29    29    13    36    14    18    19     2     0     0\n",
       "     1    39    10     5    27     5    12    13    26     2     0     0     0\n",
       "     1    50    11    10     5    29    13    18     7     2     0     0     0\n",
       " \n",
       " Columns 13 to 21 \n",
       "     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0     0     0     0\n",
       " [torch.LongTensor of size 5x22], 'x_seq_lengths': Variable containing:\n",
       "   6\n",
       "  12\n",
       "  11\n",
       "  10\n",
       "  10\n",
       " [torch.LongTensor of size 5], 'y_target': Variable containing:\n",
       "  14\n",
       "   1\n",
       "   9\n",
       "   1\n",
       "   1\n",
       " [torch.LongTensor of size 5]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_server.serve_batches(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
